{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12211534,"sourceType":"datasetVersion","datasetId":7692722}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:34:29.037039Z","iopub.execute_input":"2025-06-19T00:34:29.037325Z","iopub.status.idle":"2025-06-19T00:34:29.312505Z","shell.execute_reply.started":"2025-06-19T00:34:29.037283Z","shell.execute_reply":"2025-06-19T00:34:29.311901Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/ananyadisease-prediction-resources/Final_Augmented_dataset_Diseases_and_Symptoms.csv\n/kaggle/input/ananyadisease-prediction-resources/Disease precaution.csv\n/kaggle/input/ananyadisease-prediction-resources/model_xgboost_v3.joblib\n/kaggle/input/ananyadisease-prediction-resources/label_encoder_xgboost_v3.joblib\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import necessary libraries ‚Äî already available on Kaggle\nfrom sentence_transformers import SentenceTransformer\nfrom xgboost import XGBClassifier\n\n# Use RapidFuzz instead of fuzzywuzzy[speedup] (Kaggle doesn't support [speedup])\ntry:\n    from rapidfuzz import process, fuzz  # ‚úÖ Fast and Kaggle-compatible\nexcept ImportError:\n    from fuzzywuzzy import process, fuzz  # Fallback (slower)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:34:29.313642Z","iopub.execute_input":"2025-06-19T00:34:29.313932Z","iopub.status.idle":"2025-06-19T00:34:56.975347Z","shell.execute_reply.started":"2025-06-19T00:34:29.313915Z","shell.execute_reply":"2025-06-19T00:34:56.974608Z"}},"outputs":[{"name":"stderr","text":"2025-06-19 00:34:42.934974: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750293283.119823      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750293283.168996      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport json\nimport re\nfrom sklearn.feature_selection import VarianceThreshold\n\n# üóÇÔ∏è Load dataset (make sure it's in your Kaggle dataset folder)\ndf = pd.read_csv(\"/kaggle/input/ananyadisease-prediction-resources/Final_Augmented_dataset_Diseases_and_Symptoms.csv\")\n\n# üßπ Sanitize column names (same as used in training)\ndef sanitize_name(name):\n    name = re.sub(r'[^a-zA-Z0-9\\s]', '_', name)\n    name = re.sub(r'\\s+', '_', name)\n    return name.strip('_')\n\nX = df.drop(\"diseases\", axis=1)\nX.columns = [sanitize_name(col) for col in X.columns]\n\n# üîç Apply VarianceThreshold\nselector = VarianceThreshold(threshold=0.01)\nX_selected = selector.fit_transform(X)\nselected_features = X.columns[selector.get_support()].tolist()\n\n# üíæ Save selected features to JSON in /kaggle/working\nwith open(\"/kaggle/working/selected_features.json\", \"w\") as f:\n    json.dump(selected_features, f)\n\nprint(f\"‚úÖ selected_features.json created with {len(selected_features)} features in /kaggle/working/\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:34:56.976235Z","iopub.execute_input":"2025-06-19T00:34:56.976895Z","iopub.status.idle":"2025-06-19T00:35:05.101116Z","shell.execute_reply.started":"2025-06-19T00:34:56.976873Z","shell.execute_reply":"2025-06-19T00:35:05.100414Z"}},"outputs":[{"name":"stdout","text":"‚úÖ selected_features.json created with 146 features in /kaggle/working/\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ================================\n# STEP 0: Imports & Setup\n# ================================\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport torch\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.preprocessing import LabelEncoder\nimport re\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ================================\n# STEP 1: Load Existing Model & Data\n# ================================\nmodel = joblib.load(\"/kaggle/input/ananyadisease-prediction-resources/model_xgboost_v3.joblib\")\nlabel_encoder = joblib.load(\"/kaggle/input/ananyadisease-prediction-resources/label_encoder_xgboost_v3.joblib\")\n\naugmented_df = pd.read_csv(\"/kaggle/input/ananyadisease-prediction-resources/Final_Augmented_dataset_Diseases_and_Symptoms.csv\")\nprecaution_df = pd.read_csv(\"/kaggle/input/ananyadisease-prediction-resources/Disease precaution.csv\")\n\nwith open(\"/kaggle/working/selected_features.json\") as f:\n    valid_symptoms = json.load(f)\n\n# Extract original symptoms (before sanitization)\noriginal_symptoms = augmented_df.columns.tolist()\noriginal_symptoms.remove(\"diseases\")\n\n# ================================\n# STEP 2: Symptom Embedding\n# ================================\nembed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\nsymptom_embeddings = embed_model.encode(original_symptoms, convert_to_tensor=True)\n\n# ================================\n# STEP 3: Matching Function\n# ================================\ndef match_symptoms_with_embeddings(user_inputs, symptom_embeddings, original_symptoms, threshold=0.55):\n    matched = []\n    for inp in user_inputs:\n        query_embedding = embed_model.encode(inp, convert_to_tensor=True)\n        cos_scores = util.cos_sim(query_embedding, symptom_embeddings)[0]\n        top_idx = torch.argmax(cos_scores).item()\n        top_score = cos_scores[top_idx].item()\n\n        if top_score >= threshold:\n            matched_symptom = original_symptoms[top_idx]\n            matched.append(matched_symptom)\n            print(f\"‚úîÔ∏è Matched '{inp}' ‚Üí '{matched_symptom}' (score: {top_score:.2f})\")\n        else:\n            print(f\"‚ùå No match found for '{inp}' (score: {top_score:.2f})\")\n    return matched\n\n# ================================\n# STEP 4: Precaution Lookup\n# ================================\nfrom fuzzywuzzy import process\n\ndef get_precautions(disease_name):\n    candidates = precaution_df['Disease'].dropna().tolist()\n    match = process.extractOne(disease_name, candidates, score_cutoff=60)\n    if match:\n        matched_disease = match[0]\n        row = precaution_df[precaution_df['Disease'] == matched_disease]\n        if not row.empty:\n            return row.iloc[0][['Precaution_1', 'Precaution_2', 'Precaution_3', 'Precaution_4']].dropna().tolist()\n    return [\"No precautions found.\"]\n\n# ================================\n# STEP 5: Prediction Function\n# ================================\ndef sanitize_name(name):\n    name = re.sub(r'[^a-zA-Z0-9\\s]', '_', name)\n    name = re.sub(r'\\s+', '_', name)\n    return name.strip('_')\n\ndef predict_disease_v2(user_symptoms):\n    matched = match_symptoms_with_embeddings(user_symptoms, symptom_embeddings, original_symptoms)\n\n    if not matched:\n        return {\"Predicted Disease\": \"None\", \"Confidence\": 0.0, \"Precautions\": [\"No valid symptoms\"], \"Top Predictions\": []}\n\n    matched_clean = [sanitize_name(s) for s in matched]\n\n    vector = pd.Series(0, index=valid_symptoms)\n    for symptom in matched_clean:\n        if symptom in vector.index:\n            vector[symptom] = 1\n\n    vector = vector.values.reshape(1, -1)\n    y_pred = model.predict(vector)[0]\n    probs = model.predict_proba(vector)[0]\n\n    predicted = label_encoder.inverse_transform([y_pred])[0]\n    confidence = probs[y_pred]\n\n    top_indices = np.argsort(probs)[-3:][::-1]\n    top_3 = [(label_encoder.inverse_transform([i])[0], probs[i]) for i in top_indices]\n\n    return {\n        \"Predicted Disease\": predicted,\n        \"Confidence\": round(confidence, 3),\n        \"Top Predictions\": [(d, round(p, 3)) for d, p in top_3],\n        \"Precautions\": get_precautions(predicted)\n    }\n\n# ================================\n# STEP 6: Test Example\n# ================================\ntest_input = [\"my head hurts\", \"dizzy\", \"burning in chest\"]\nprint(predict_disease_v2(test_input))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:36:00.718041Z","iopub.execute_input":"2025-06-19T00:36:00.718357Z","iopub.status.idle":"2025-06-19T00:36:17.765800Z","shell.execute_reply.started":"2025-06-19T00:36:00.718336Z","shell.execute_reply":"2025-06-19T00:36:17.765019Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0754873a16748a4a0c40f1435e2feb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99ce849ed5164346a66f0e5d29707dc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4f8d74af8734428a01614018ea94fc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a803a0181c024663962efd5c01a8348c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25a53e31fc574e33b044e60e36366181"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"305298c1d8cf467d904ee0b07b87b027"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eb0db78a22b4e678b083be0a0ce06a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1371e5f3a8d14b2da81c3fd366429763"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5799aa5b50674210a22460a17a0cb40a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aecd3ad713184cf1a7c628761d2d616a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06b86caa74164e0bbda3176d28523a14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"024fe9ddd05445a896996a24147e854c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef3f3820bb5341ce85677fe931bf0edf"}},"metadata":{}},{"name":"stdout","text":"‚úîÔ∏è Matched 'my head hurts' ‚Üí 'headache' (score: 0.65)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38b52a10dcb64f99b6937d2d9a844d78"}},"metadata":{}},{"name":"stdout","text":"‚úîÔ∏è Matched 'dizzy' ‚Üí 'dizziness' (score: 0.81)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afb9e62395d74f6598f2e161e61e7b4d"}},"metadata":{}},{"name":"stdout","text":"‚úîÔ∏è Matched 'burning in chest' ‚Üí 'burning chest pain' (score: 0.89)\n{'Predicted Disease': 'tension headache', 'Confidence': 0.186, 'Top Predictions': [('tension headache', 0.186), ('autonomic nervous system disorder', 0.107), ('trigeminal neuralgia', 0.06)], 'Precautions': ['bath twice', 'avoid fatty spicy food', 'drink plenty of water', 'avoid too many products']}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ================================\n# STEP 0: Install Dependencies (Kaggle usually already has them)\n# ================================\n# You don't need !pip install commands in Kaggle if packages already exist.\n# For completeness, uncomment if needed:\n# !pip install sentence-transformers scikit-learn tensorflow pandas\n\n# ================================\n# STEP 1: Import Libraries\n# ================================\nimport pandas as pd\nimport numpy as np\nimport json\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\n# ================================\n# STEP 2: Load Dataset\n# ================================\ndf = pd.read_csv(\"/kaggle/input/ananyadisease-prediction-resources/Final_Augmented_dataset_Diseases_and_Symptoms.csv\")\ndf = df.dropna(subset=[\"diseases\"])\n\n# ================================\n# STEP 3: Convert Symptom Indicators into Sentences\n# ================================\nsymptom_columns = df.columns.drop(\"diseases\")\ndf[\"symptom_sentence\"] = df[symptom_columns].apply(\n    lambda row: \", \".join([col.replace(\"_\", \" \") for col in symptom_columns if row[col] == 1]), axis=1\n)\n\n# ================================\n# STEP 4: Generate Sentence Embeddings\n# ================================\nembedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\nX_embeddings = embedder.encode(df[\"symptom_sentence\"].tolist(), show_progress_bar=True)\n\n# ================================\n# STEP 5: Encode Target Labels\n# ================================\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(df[\"diseases\"])\ny_onehot = to_categorical(y_encoded)\nnum_classes = y_onehot.shape[1]\n\n# ================================\n# STEP 6: Train-Test Split\n# ================================\nX_train, X_test, y_train, y_test = train_test_split(X_embeddings, y_onehot, test_size=0.2, random_state=42)\n\n# ================================\n# STEP 7: Define and Train Neural Network\n# ================================\nmodel = Sequential([\n    Dense(256, activation='relu', input_shape=(X_embeddings.shape[1],)),\n    Dropout(0.3),\n    Dense(128, activation='relu'),\n    Dropout(0.2),\n    Dense(num_classes, activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n\n# ================================\n# STEP 8: Save Model and Encoder\n# ================================\nmodel.save(\"/kaggle/working/disease_predictor_nn.h5\")\nwith open(\"/kaggle/working/label_encoder_nn.json\", \"w\") as f:\n    json.dump(label_encoder.classes_.tolist(), f)\n\n# ================================\n# STEP 9: Evaluate\n# ================================\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"‚úÖ Test Accuracy: {accuracy:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:37:37.640719Z","iopub.execute_input":"2025-06-19T00:37:37.641586Z","iopub.status.idle":"2025-06-19T00:43:48.174951Z","shell.execute_reply.started":"2025-06-19T00:37:37.641551Z","shell.execute_reply":"2025-06-19T00:43:48.174224Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/7718 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5c312542e674772a05fac818effc3f9"}},"metadata":{}},{"name":"stderr","text":"I0000 00:00:1750293683.857570      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15207 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1750293688.842718     123 service.cc:148] XLA service 0x7c96bc06c1f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1750293688.843569     123 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1750293689.064623     123 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  72/4939\u001b[0m \u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.0087 - loss: 6.5586     ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1750293690.508968     123 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m4939/4939\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.3385 - loss: 2.9872 - val_accuracy: 0.7496 - val_loss: 0.8124\nEpoch 2/10\n\u001b[1m4939/4939\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.6961 - loss: 0.9688 - val_accuracy: 0.8029 - val_loss: 0.5929\nEpoch 3/10\n\u001b[1m4939/4939\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.7512 - loss: 0.7656 - val_accuracy: 0.8155 - val_loss: 0.5394\nEpoch 4/10\n\u001b[1m4939/4939\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.7737 - loss: 0.6806 - val_accuracy: 0.8242 - val_loss: 0.4977\nEpoch 5/10\n\u001b[1m4939/4939\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.7877 - loss: 0.6279 - val_accuracy: 0.8322 - val_loss: 0.4648\nEpoch 6/10\n\u001b[1m4939/4939\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.7967 - loss: 0.5962 - val_accuracy: 0.8337 - val_loss: 0.4508\nEpoch 7/10\n\u001b[1m4939/4939\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.8052 - loss: 0.5636 - val_accuracy: 0.8376 - val_loss: 0.4432\nEpoch 8/10\n\u001b[1m4939/4939\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.8070 - loss: 0.5537 - val_accuracy: 0.8419 - val_loss: 0.4338\nEpoch 9/10\n\u001b[1m4939/4939\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.8134 - loss: 0.5344 - val_accuracy: 0.8446 - val_loss: 0.4215\nEpoch 10/10\n\u001b[1m4939/4939\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.8146 - loss: 0.5294 - val_accuracy: 0.8398 - val_loss: 0.4241\n\u001b[1m1544/1544\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8455 - loss: 0.4142\n‚úÖ Test Accuracy: 0.84\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\n\n# üîΩ Load dataset (update the path as per your Kaggle dataset name)\ndf = pd.read_csv(\"/kaggle/input/ananyadisease-prediction-resources/Final_Augmented_dataset_Diseases_and_Symptoms.csv\")\n\n# üí¨ Convert binary symptom indicators to natural language descriptions\ndef row_to_description(row):\n    return ', '.join([col.replace('_', ' ') for col in df.columns[1:] if row[col] == 1])\n\n# üß™ Generate the description column\ndf[\"description\"] = df.apply(row_to_description, axis=1)\n\n# üíæ Save as hybrid_dataset.csv in /kaggle/working\ndf.to_csv(\"/kaggle/working/hybrid_dataset.csv\", index=False)\n\nprint(\"‚úÖ hybrid_dataset.csv created with\", len(df), \"rows.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:43:54.941522Z","iopub.execute_input":"2025-06-19T00:43:54.941896Z","iopub.status.idle":"2025-06-19T00:46:49.473358Z","shell.execute_reply.started":"2025-06-19T00:43:54.941875Z","shell.execute_reply":"2025-06-19T00:46:49.472590Z"}},"outputs":[{"name":"stdout","text":"‚úÖ hybrid_dataset.csv created with 246945 rows.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# üì¶ Install required packages (Run this cell first in Kaggle)\n!pip install -q sentence-transformers transformers fuzzywuzzy[speedup]\n\n# üìö Imports\nimport pandas as pd\nimport numpy as np\nimport joblib\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\n# ‚úÖ Load hybrid dataset\ndata = pd.read_csv(\"/kaggle/working/hybrid_dataset.csv\")\n\n# ‚ûó Split features\nX_structured = data.drop(columns=[\"diseases\", \"description\"]).to_numpy()\nX_text = data[\"description\"].tolist()\ny = data[\"diseases\"]\n\n# üß† Sentence embeddings\nembedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\nX_embed = embedder.encode(X_text)\n\n# üîÄ Concatenate SBERT embeddings + structured symptoms\nX_combined = np.concatenate([X_structured, X_embed], axis=1)\n\n# üî° Encode labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# üß™ Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_combined, y_encoded, test_size=0.2, random_state=42)\n\n# üß† Hybrid model architecture\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(X_combined.shape[1],)),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n\n# üíæ Save model and encoder to /kaggle/working\nmodel.save(\"/kaggle/working/hybrid_nn_disease_predictor.h5\")\njoblib.dump(label_encoder, \"/kaggle/working/label_encoder_hybrid.joblib\")\n\nprint(\"‚úÖ Hybrid model and label encoder saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:49:40.857576Z","iopub.execute_input":"2025-06-19T00:49:40.858216Z","iopub.status.idle":"2025-06-19T00:53:24.417512Z","shell.execute_reply.started":"2025-06-19T00:49:40.858184Z","shell.execute_reply":"2025-06-19T00:53:24.416819Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/7718 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffa9873073ad493688f004d2d1c54bee"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m5557/5557\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.5029 - loss: 2.3319 - val_accuracy: 0.8380 - val_loss: 0.4786\nEpoch 2/10\n\u001b[1m5557/5557\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - accuracy: 0.7930 - loss: 0.6465 - val_accuracy: 0.8432 - val_loss: 0.4269\nEpoch 3/10\n\u001b[1m5557/5557\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - accuracy: 0.8141 - loss: 0.5615 - val_accuracy: 0.8514 - val_loss: 0.4018\nEpoch 4/10\n\u001b[1m5557/5557\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - accuracy: 0.8225 - loss: 0.5288 - val_accuracy: 0.8504 - val_loss: 0.3900\nEpoch 5/10\n\u001b[1m5557/5557\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.8258 - loss: 0.5076 - val_accuracy: 0.8512 - val_loss: 0.3857\nEpoch 6/10\n\u001b[1m5557/5557\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - accuracy: 0.8309 - loss: 0.4865 - val_accuracy: 0.8538 - val_loss: 0.3758\nEpoch 7/10\n\u001b[1m5557/5557\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - accuracy: 0.8329 - loss: 0.4795 - val_accuracy: 0.8561 - val_loss: 0.3684\nEpoch 8/10\n\u001b[1m5557/5557\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - accuracy: 0.8353 - loss: 0.4703 - val_accuracy: 0.8559 - val_loss: 0.3739\nEpoch 9/10\n\u001b[1m5557/5557\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.8374 - loss: 0.4625 - val_accuracy: 0.8535 - val_loss: 0.3689\nEpoch 10/10\n\u001b[1m5557/5557\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - accuracy: 0.8385 - loss: 0.4556 - val_accuracy: 0.8547 - val_loss: 0.3616\n‚úÖ Hybrid model and label encoder saved.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# üìö Imports\nimport numpy as np\nimport pandas as pd\nimport torch\nimport joblib\nfrom transformers import AutoTokenizer, AutoModel\nfrom tensorflow.keras.models import load_model\nfrom fuzzywuzzy import process\n\n# üß† Load SBERT\nmodel_name = \"sentence-transformers/all-MiniLM-L6-v2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nsbert_model = AutoModel.from_pretrained(model_name)\n\ndef sentence_to_embedding(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n    with torch.no_grad():\n        output = sbert_model(**inputs).last_hidden_state.mean(dim=1)\n    return output.squeeze().numpy()\n\n# üì¶ Load trained model & encoder\nmodel = load_model(\"/kaggle/working/hybrid_nn_disease_predictor.h5\")\nlabel_encoder = joblib.load(\"/kaggle/working/label_encoder_hybrid.joblib\")\n\n# üìñ Load precautions\nprecautions_df = pd.read_csv(\"/kaggle/input/ananyadisease-prediction-resources/Disease precaution.csv\")\n\n# üß™ Final prediction function\ndef predict_disease(text):\n    emb = sentence_to_embedding(text).reshape(1, -1)\n\n    # Zero vector for structured input (if you want to add later)\n    dummy_structured = np.zeros((1, model.input_shape[1] - emb.shape[1]))\n    full_input = np.concatenate([dummy_structured, emb], axis=1)\n\n    probs = model.predict(full_input)[0]\n    top_indices = probs.argsort()[-3:][::-1]\n    top_preds = [(label_encoder.inverse_transform([i])[0], float(probs[i])) for i in top_indices]\n    top_disease = top_preds[0][0]\n\n    # Precaution match\n    match = process.extractOne(top_disease, precautions_df[\"Disease\"], score_cutoff=60)\n    if match:\n        row = precautions_df[precautions_df[\"Disease\"] == match[0]]\n        precautions = row.iloc[0, 1:].dropna().tolist()\n    else:\n        precautions = [\"No specific precautions found.\"]\n\n    return {\n        \"Predicted Disease\": top_disease,\n        \"Confidence\": float(probs[top_indices[0]]),\n        \"Top Predictions\": top_preds,\n        \"Precautions\": precautions\n    }\n\n# üîç Example usage\nprint(predict_disease(\"headache,running nose, feverish\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T00:54:31.306220Z","iopub.execute_input":"2025-06-19T00:54:31.306565Z","iopub.status.idle":"2025-06-19T00:54:32.011400Z","shell.execute_reply.started":"2025-06-19T00:54:31.306544Z","shell.execute_reply":"2025-06-19T00:54:32.010711Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307ms/step\n{'Predicted Disease': 'common cold', 'Confidence': 0.8416118025779724, 'Top Predictions': [('common cold', 0.8416118025779724), ('strep throat', 0.03669017180800438), ('acute sinusitis', 0.027172205969691277)], 'Precautions': ['drink vitamin c rich drinks', 'take vapour', 'avoid cold food', 'keep fever in check']}\n","output_type":"stream"}],"execution_count":11}]}
